# Hugging-face
## In Hugging face I have used facebook/bart-large-cnn, It is used for text summarization. A pipeline is being created for this purpose.
## In pegasus-xsum I have used google/pegasus-xsum, It is also a technique for text summariztion and it uses Tokenization method to initialize the the token and after applying google/pegasus-xsum. It detokenize and returns the result.
## Transformers are used in both of the approaches
## Transformer is a deep learning model that adopts the mechanism of self attention. Self attention is a concept of enhancing of some parts of input data while deminishing the other parts. It is mostly used NLP and Computer Vision.
